# -*- coding: utf-8 -*-
"""Breast_Cancer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pkUUIwzMUuzAWPcdhRq1EcN6Qz5N5Gpf

# Description: This program detects breast cancer, based off of data
"""

# import libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the data
# from google.colab import files
# uploaded = files.upload()
df = pd.read_csv("wdbc.data")
df.tail(7)

# Count the number for rows and columns in the dataset
df.shape

# Count the number of empty values(Nan, NAN, na, 0) in each column
df.isna().sum()

# drop the unnamed column having all missing values
df = df.dropna(axis=1)

# Get new count of the number of rows and columns
df.shape

# Get a count of the number of malignant(M) or benign(B) cells
df['diagnosis'].value_counts()

# Visualize the count
sns.countplot(df['diagnosis'], label='count')

# Look at the datatypes to see that need to be encoded
df.dtypes

df.iloc[:,1].values

# Encode the categorical data values
from sklearn.preprocessing import LabelEncoder
labelencoder_Y = LabelEncoder()
# It will make M = 1 and B = 0
df.iloc[:,1] = labelencoder_Y.fit_transform(df.iloc[:,1].values)

# Create a pair plot
sns.pairplot(df.iloc[:,1:6], hue="diagnosis")

# Print the first 5 rows of the new data
df.head()

# Get the correlation of the columns
df.iloc[:, 1:12].corr()

# Visualize the correlation
plt.figure(figsize=(10,10))
sns.heatmap(df.iloc[:, 1:12].corr(), annot=True, fmt='.0%')

# Split the dataset into independent (X) and dependent (Y) data sets
X = df.iloc[:,2:31].values
Y = df.iloc[:,1].values

# Split the dataset into 75% training and 25% testing
from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25,
                                                    random_state=0)

# Scale the data (Feature scaling)
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.fit_transform(X_test)

# Create a function for the models
def models(X_train, Y_train):
  
  # Logistic regression model
  from sklearn.linear_model import LogisticRegression
  log = LogisticRegression(random_state=0)
  log.fit(X_train, Y_train)

  # Decision Tree
  from sklearn.tree import DecisionTreeClassifier
  tree = DecisionTreeClassifier(criterion='entropy', random_state=0)
  tree.fit(X_train, Y_train)

  # Random Forest Classifier
  from sklearn.ensemble import RandomForestClassifier
  forest = RandomForestClassifier(n_estimators = 10, criterion='entropy', random_state=0)
  forest.fit(X_train, Y_train)

  # Print the model accuracy on the training data
  print("[0]Logistic Regression Training accuracy: ", log.score(X_train, Y_train))
  print("[1]Decision Tree Classifier Training accuracy: ", tree.score(X_train, Y_train))
  print("[2]Random Forest Classifier Training accuracy: ", forest.score(X_train, Y_train))

  return log, tree, forest

# Getting all of the models
model = models(X_train, Y_train)

# Test model accuracy on confusion matrix
from sklearn.metrics import confusion_matrix
for i in range(len(model)):
  print(f"Model {i}")
  cm = confusion_matrix(Y_test, model[i].predict(X_test))

  TP = cm[0][0]
  TN = cm[1][1]
  FN = cm[1][0]
  FP = cm[0][1]

  print(cm)
  print("Testing Accuracy: ", (TP+TN)/(TP+TN+FN+FP))
  print()

# Another way to get metrics of the models
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
for i in range(len(model)):
  print(f"Model {i}")
  print(classification_report(Y_test, model[i].predict(X_test)))
  print(accuracy_score(Y_test, model[i].predict(X_test)))
  print()

# Print the prediction of random forest classifier model
pred = model[2].predict(X_test)
print(pred)
print()
print(Y_test)